{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h1 align=\"center\">Crash Course on NLP using Tensorflow</h1>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer   ## Generate dictionary of word encodings\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [\"I love my dog\",\n            \"I love my cat\"]\n\ntokenizer = Tokenizer(num_words = 100)   ## Hyperparameter to select the top 100 words by volume\ntokenizer.fit_on_texts(sentences)        ## Encodes the data\nword_index = tokenizer.word_index        ## return a dictionary\nprint(word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If another sentence would be added, with an excalimation mark then it will be not be counted as a different word. Also cleans space and case sensitivites."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [\"I love my dog\",\n            \"i love my cat\",\n            \"You love my dog!\"]\n\ntokenizer = Tokenizer(num_words = 100)   ## num_words hyperparameter to select the top 100 unique words by volume\ntokenizer.fit_on_texts(sentences)        ## Encodes the data\nword_index = tokenizer.word_index        ## return a dictionary\nprint(word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**The words being encoded into numbers will help in their use with neural networks**"},{"metadata":{},"cell_type":"markdown","source":"## Text to Sequence"},{"metadata":{},"cell_type":"markdown","source":"```texts_to_sequences``` will produce the sequence on the same word encoding it has been fitted on or else it will be meaning less. Words which have not been previously encoded to the word index will be lost in sequencing."},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = [\"I love my dog\",\n            \"i love my cat\",\n            \"You love my dog!\",\n            \"Do you think my dog is amazing?\"]\n\ntokenizer = Tokenizer(num_words = 100)   ## num_words hyperparameter to select the top 100 unique words by volume\ntokenizer.fit_on_texts(sentences)        ## Encodes the data\nword_index = tokenizer.word_index        ## return a dictionary\nprint(word_index,\"\\n\")\n\nsequences = tokenizer.texts_to_sequences(sentences)    ## Generate the sentence sequnces usingthe encodings\nprint(sequences)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also replace a n unseen word with a special value with a property of the tokenizer called `oov_token = \"<OOv>\"` to use outer vocabulary words."},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = Tokenizer(num_words = 100, oov_token = \"<OOV>\")   ## num_words selects the top 100 unique words by volume and ovv is for outer vocabulary place holder\ntokenizer.fit_on_texts(sentences)        ## Encodes the data\nword_index = tokenizer.word_index        ## return a dictionary\nprint(word_index,\"\\n\")\n\ntest_data = [\"I really love my dog\",\n            \"My dog love his food\"]\ntest_seq = tokenizer.texts_to_sequences(test_data)\nprint(test_seq)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here unknown words are represented as 1"},{"metadata":{},"cell_type":"markdown","source":"## Padding\nBefore we can train with texts we need some level of uniformity of size. This is why padding is required"},{"metadata":{"trusted":true},"cell_type":"code","source":"padded  = pad_sequences(sequences , padding=\"post\", maxlen = 5, truncating = \"post\")\nprint(padded)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each row of the matrix has equal lengths.\n1. If you want the padding to be aaplied at the end of the sentence then set `padding = \"post\"`\n2. If you want to limit the size then set `maxlen` accordingly. By deafult the deletion will be pre\n3. To make the `maxlen` delete from post set `truncating = \"post\"`"},{"metadata":{},"cell_type":"markdown","source":"# Sarcasam Detector (a small experimentation)\n> Sarcasm Detection using Hybrid Neural Network\nRishabh Misra, Prahal Arora\nArxiv, August 2019"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\n\nwith open(\"../input/sarcasm-data-news-headlines/sarcasm.json\", 'r') as f:\n    datastore = json.load(f)\n\n\nheadlines = [] \nlabels = []\nurls = []\nfor i in datastore:                    ## Itterate over the elements\n    headlines.append(i['headline'])\n    labels.append(i['is_sarcastic'])\n    urls.append(i['article_link'])\n\ntokenizer = Tokenizer(oov_token=\"<OOV>\")    ## Most of the common words like in,to,and will be present prety high up the list\ntokenizer.fit_on_texts(headlines)\n\nword_index = tokenizer.word_index\nprint(len(word_index))\nprint(word_index)\nsequences = tokenizer.texts_to_sequences(headlines)\npadded = pad_sequences(sequences, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"The sentence is: \",sentences[2],\"\\n\")\nprint(\"The sentence sequence is: \",padded[2],\"\\n\")\nprint(\"Shape of padded array (no. of sentences X words in each): \",padded.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}